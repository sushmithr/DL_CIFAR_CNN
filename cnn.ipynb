{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of cnn.ipynb","version":"0.3.2","provenance":[{"file_id":"1SKjjkEhIw22RDMC9yqp688M8ubCaPaHv","timestamp":1554684039993}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"UUPAkjpLxMfU","colab_type":"text"},"cell_type":"markdown","source":["# Home 3: Build a CNN for image recognition.\n","\n","### Name: Sushmith Ramesh\n"]},{"metadata":{"id":"9AMoE7RhxMfW","colab_type":"text"},"cell_type":"markdown","source":["## 0. You will do the following:\n","\n","1. Read, complete, and run my code.\n","\n","2. **Make substantial improvements** to maximize the accurcy.\n","    \n","3. Convert the .IPYNB file to .HTML file.\n","\n","    * The HTML file must contain the code and the output after execution.\n","    \n","4. Upload this .HTML file to your Github repo.\n","\n","4. Submit the link to this .HTML file to Canvas.\n","\n","    * Example: https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM3/cnn.html\n"]},{"metadata":{"id":"x-OL_y9vxMfY","colab_type":"text"},"cell_type":"markdown","source":["## 1. Data preparation"]},{"metadata":{"id":"LGXYtylXxMfZ","colab_type":"text"},"cell_type":"markdown","source":["### 1.1. Load data\n"]},{"metadata":{"id":"yPF9Hn1JxMfb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":102},"outputId":"7e293924-3403-4e1d-a5ac-2639781aae27","executionInfo":{"status":"ok","timestamp":1554672502727,"user_tz":240,"elapsed":1852,"user":{"displayName":"Sushmith Ramesh","photoUrl":"https://lh3.googleusercontent.com/-bVb_Tqiu7NU/AAAAAAAAAAI/AAAAAAAAAqU/mfZ6cPSstVg/s64/photo.jpg","userId":"13694073694977092311"}}},"cell_type":"code","source":["from keras.datasets import cifar10\n","import numpy\n","\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","x_train = x_train.astype('float32')\n","x_test = x_test.astype('float32')\n","\n","mean = numpy.mean(x_train,axis=(0,1,2,3))\n","std = numpy.std(x_train,axis=(0,1,2,3))\n","x_train = (x_train-mean)/(std+1e-7)\n","x_test = (x_test-mean)/(std+1e-7)\n","\n","print('shape of x_train: ' + str(x_train.shape))\n","print('shape of y_train: ' + str(y_train.shape))\n","print('shape of x_test: ' + str(x_test.shape))\n","print('shape of y_test: ' + str(y_test.shape))\n","print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))"],"execution_count":56,"outputs":[{"output_type":"stream","text":["shape of x_train: (50000, 32, 32, 3)\n","shape of y_train: (50000, 1)\n","shape of x_test: (10000, 32, 32, 3)\n","shape of y_test: (10000, 1)\n","number of classes: 10\n"],"name":"stdout"}]},{"metadata":{"id":"dTdOWA_vxMfk","colab_type":"text"},"cell_type":"markdown","source":["### 1.2. One-hot encode the labels\n","\n","In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n","\n","1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n","\n","2. Apply the function to ```y_train``` and ```y_test```."]},{"metadata":{"id":"OxmWAwiqxMfl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"47404004-a52b-404a-b49a-8bb4847974ec","executionInfo":{"status":"ok","timestamp":1554672508460,"user_tz":240,"elapsed":599,"user":{"displayName":"Sushmith Ramesh","photoUrl":"https://lh3.googleusercontent.com/-bVb_Tqiu7NU/AAAAAAAAAAI/AAAAAAAAAqU/mfZ6cPSstVg/s64/photo.jpg","userId":"13694073694977092311"}}},"cell_type":"code","source":["def to_one_hot(y, num_class=10):\n","    results = numpy.zeros((len(y), num_class))\n","    for i, label in enumerate(y):\n","        results[i, label] = 1\n","    return results\n","\n","y_train_vec = to_one_hot(y_train)\n","y_test_vec = to_one_hot(y_test)\n","\n","print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n","print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n","\n","print(y_train[0])\n","print(y_train_vec[0])"],"execution_count":57,"outputs":[{"output_type":"stream","text":["Shape of y_train_vec: (50000, 10)\n","Shape of y_test_vec: (10000, 10)\n","[6]\n","[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"],"name":"stdout"}]},{"metadata":{"id":"nIMuq5dGxMfq","colab_type":"text"},"cell_type":"markdown","source":["#### Remark: the outputs should be\n","* Shape of y_train_vec: (50000, 10)\n","* Shape of y_test_vec: (10000, 10)\n","* [6]\n","* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"]},{"metadata":{"id":"pN4ino0CxMfs","colab_type":"text"},"cell_type":"markdown","source":["### 1.3. Randomly partition the training set to training and validation sets\n","\n","Randomly partition the 50K training samples to 2 sets:\n","* a training set containing 40K samples\n","* a validation set containing 10K samples\n"]},{"metadata":{"id":"6qA1_dCmxMfs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"97b9a9fa-f807-4707-d913-b53a54c07520","executionInfo":{"status":"ok","timestamp":1554672514541,"user_tz":240,"elapsed":944,"user":{"displayName":"Sushmith Ramesh","photoUrl":"https://lh3.googleusercontent.com/-bVb_Tqiu7NU/AAAAAAAAAAI/AAAAAAAAAqU/mfZ6cPSstVg/s64/photo.jpg","userId":"13694073694977092311"}}},"cell_type":"code","source":["rand_indices = numpy.random.permutation(50000)\n","train_indices = rand_indices[0:40000]\n","valid_indices = rand_indices[40000:50000]\n","\n","x_val = x_train[valid_indices, :]\n","y_val = y_train_vec[valid_indices, :]\n","\n","x_tr = x_train[train_indices, :]\n","y_tr = y_train_vec[train_indices, :]\n","\n","print('Shape of x_tr: ' + str(x_tr.shape))\n","print('Shape of y_tr: ' + str(y_tr.shape))\n","print('Shape of x_val: ' + str(x_val.shape))\n","print('Shape of y_val: ' + str(y_val.shape))"],"execution_count":58,"outputs":[{"output_type":"stream","text":["Shape of x_tr: (40000, 32, 32, 3)\n","Shape of y_tr: (40000, 10)\n","Shape of x_val: (10000, 32, 32, 3)\n","Shape of y_val: (10000, 10)\n"],"name":"stdout"}]},{"metadata":{"id":"laEFrHzGxMfw","colab_type":"text"},"cell_type":"markdown","source":["## 2. Build a CNN and tune its hyper-parameters\n","\n","1. Build a convolutional neural network model\n","2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n","    * Do NOT use test data for hyper-parameter tuning!!!\n","3. Try to achieve a validation accuracy as high as possible."]},{"metadata":{"id":"ijBPaN52xMfx","colab_type":"text"},"cell_type":"markdown","source":["### Remark: \n","\n","The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n","* Add more layers.\n","* Use regularizations, e.g., dropout.\n","* Use batch normalization."]},{"metadata":{"id":"yX2S4lhZxMfy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1088},"outputId":"33c7bd64-a0ad-4dcd-af67-dfd15583f9b8","executionInfo":{"status":"ok","timestamp":1554672525036,"user_tz":240,"elapsed":1521,"user":{"displayName":"Sushmith Ramesh","photoUrl":"https://lh3.googleusercontent.com/-bVb_Tqiu7NU/AAAAAAAAAAI/AAAAAAAAAqU/mfZ6cPSstVg/s64/photo.jpg","userId":"13694073694977092311"}}},"cell_type":"code","source":["from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\n","from keras import regularizers\n","from keras.models import Sequential\n","\n","weight_decay = 1e-4\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=(32, 32, 3)))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D((2, 2)))\n","\n","model.add(Dropout(0.2))\n","\n","model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D((2, 2)))\n","\n","model.add(Dropout(0.3))\n","\n","model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D((2, 2)))\n","\n","model.add(Dropout(0.4))\n","\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dense(10, activation='softmax'))\n","\n","model.summary()"],"execution_count":59,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_51 (Conv2D)           (None, 32, 32, 32)        896       \n","_________________________________________________________________\n","batch_normalization_63 (Batc (None, 32, 32, 32)        128       \n","_________________________________________________________________\n","activation_51 (Activation)   (None, 32, 32, 32)        0         \n","_________________________________________________________________\n","conv2d_52 (Conv2D)           (None, 32, 32, 32)        9248      \n","_________________________________________________________________\n","batch_normalization_64 (Batc (None, 32, 32, 32)        128       \n","_________________________________________________________________\n","activation_52 (Activation)   (None, 32, 32, 32)        0         \n","_________________________________________________________________\n","max_pooling2d_48 (MaxPooling (None, 16, 16, 32)        0         \n","_________________________________________________________________\n","dropout_15 (Dropout)         (None, 16, 16, 32)        0         \n","_________________________________________________________________\n","conv2d_53 (Conv2D)           (None, 16, 16, 64)        18496     \n","_________________________________________________________________\n","batch_normalization_65 (Batc (None, 16, 16, 64)        256       \n","_________________________________________________________________\n","activation_53 (Activation)   (None, 16, 16, 64)        0         \n","_________________________________________________________________\n","conv2d_54 (Conv2D)           (None, 16, 16, 64)        36928     \n","_________________________________________________________________\n","batch_normalization_66 (Batc (None, 16, 16, 64)        256       \n","_________________________________________________________________\n","activation_54 (Activation)   (None, 16, 16, 64)        0         \n","_________________________________________________________________\n","max_pooling2d_49 (MaxPooling (None, 8, 8, 64)          0         \n","_________________________________________________________________\n","dropout_16 (Dropout)         (None, 8, 8, 64)          0         \n","_________________________________________________________________\n","conv2d_55 (Conv2D)           (None, 8, 8, 128)         73856     \n","_________________________________________________________________\n","batch_normalization_67 (Batc (None, 8, 8, 128)         512       \n","_________________________________________________________________\n","activation_55 (Activation)   (None, 8, 8, 128)         0         \n","_________________________________________________________________\n","conv2d_56 (Conv2D)           (None, 8, 8, 128)         147584    \n","_________________________________________________________________\n","batch_normalization_68 (Batc (None, 8, 8, 128)         512       \n","_________________________________________________________________\n","activation_56 (Activation)   (None, 8, 8, 128)         0         \n","_________________________________________________________________\n","max_pooling2d_50 (MaxPooling (None, 4, 4, 128)         0         \n","_________________________________________________________________\n","dropout_17 (Dropout)         (None, 4, 4, 128)         0         \n","_________________________________________________________________\n","flatten_13 (Flatten)         (None, 2048)              0         \n","_________________________________________________________________\n","dense_25 (Dense)             (None, 128)               262272    \n","_________________________________________________________________\n","batch_normalization_69 (Batc (None, 128)               512       \n","_________________________________________________________________\n","dense_26 (Dense)             (None, 10)                1290      \n","=================================================================\n","Total params: 552,874\n","Trainable params: 551,722\n","Non-trainable params: 1,152\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"b5oV8z1pxMf0","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras import optimizers\n","\n","learning_rate = 1E-4 # to be tuned!\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=optimizers.RMSprop(lr=learning_rate),\n","              metrics=['acc'])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tXO9nMdpxMf3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":3434},"outputId":"b6b8f111-9ffc-4e6e-94c4-cc419804e6a7","executionInfo":{"status":"ok","timestamp":1554674921774,"user_tz":240,"elapsed":2387096,"user":{"displayName":"Sushmith Ramesh","photoUrl":"https://lh3.googleusercontent.com/-bVb_Tqiu7NU/AAAAAAAAAAI/AAAAAAAAAqU/mfZ6cPSstVg/s64/photo.jpg","userId":"13694073694977092311"}}},"cell_type":"code","source":["history = model.fit(x_tr, y_tr, batch_size=64, epochs=100, validation_data=(x_val, y_val))"],"execution_count":61,"outputs":[{"output_type":"stream","text":["Train on 40000 samples, validate on 10000 samples\n","Epoch 1/100\n","40000/40000 [==============================] - 32s 811us/step - loss: 2.0219 - acc: 0.3067 - val_loss: 1.6084 - val_acc: 0.4054\n","Epoch 2/100\n","40000/40000 [==============================] - 25s 637us/step - loss: 1.5187 - acc: 0.4622 - val_loss: 1.4692 - val_acc: 0.4740\n","Epoch 3/100\n","40000/40000 [==============================] - 25s 623us/step - loss: 1.3375 - acc: 0.5320 - val_loss: 1.2750 - val_acc: 0.5526\n","Epoch 4/100\n","40000/40000 [==============================] - 25s 625us/step - loss: 1.2101 - acc: 0.5823 - val_loss: 1.2196 - val_acc: 0.5674\n","Epoch 5/100\n","40000/40000 [==============================] - 25s 623us/step - loss: 1.1117 - acc: 0.6177 - val_loss: 1.1510 - val_acc: 0.6027\n","Epoch 6/100\n","40000/40000 [==============================] - 25s 622us/step - loss: 1.0310 - acc: 0.6498 - val_loss: 0.9719 - val_acc: 0.6648\n","Epoch 7/100\n","40000/40000 [==============================] - 24s 609us/step - loss: 0.9725 - acc: 0.6677 - val_loss: 1.0370 - val_acc: 0.6369\n","Epoch 8/100\n","40000/40000 [==============================] - 24s 606us/step - loss: 0.9241 - acc: 0.6854 - val_loss: 0.8623 - val_acc: 0.7064\n","Epoch 9/100\n","40000/40000 [==============================] - 24s 606us/step - loss: 0.8866 - acc: 0.6972 - val_loss: 0.8866 - val_acc: 0.7016\n","Epoch 10/100\n","40000/40000 [==============================] - 24s 589us/step - loss: 0.8541 - acc: 0.7113 - val_loss: 0.7871 - val_acc: 0.7362\n","Epoch 11/100\n","40000/40000 [==============================] - 24s 590us/step - loss: 0.8259 - acc: 0.7222 - val_loss: 0.8125 - val_acc: 0.7269\n","Epoch 12/100\n","40000/40000 [==============================] - 24s 591us/step - loss: 0.7982 - acc: 0.7308 - val_loss: 0.7698 - val_acc: 0.7420\n","Epoch 13/100\n","40000/40000 [==============================] - 24s 592us/step - loss: 0.7673 - acc: 0.7437 - val_loss: 0.7432 - val_acc: 0.7535\n","Epoch 14/100\n","40000/40000 [==============================] - 24s 591us/step - loss: 0.7511 - acc: 0.7484 - val_loss: 0.7885 - val_acc: 0.7334\n","Epoch 15/100\n","40000/40000 [==============================] - 24s 592us/step - loss: 0.7343 - acc: 0.7531 - val_loss: 0.7099 - val_acc: 0.7630\n","Epoch 16/100\n","40000/40000 [==============================] - 24s 591us/step - loss: 0.7139 - acc: 0.7611 - val_loss: 0.7108 - val_acc: 0.7608\n","Epoch 17/100\n","40000/40000 [==============================] - 24s 591us/step - loss: 0.6939 - acc: 0.7686 - val_loss: 0.6714 - val_acc: 0.7758\n","Epoch 18/100\n","40000/40000 [==============================] - 24s 590us/step - loss: 0.6803 - acc: 0.7723 - val_loss: 0.6813 - val_acc: 0.7725\n","Epoch 19/100\n","40000/40000 [==============================] - 24s 589us/step - loss: 0.6605 - acc: 0.7804 - val_loss: 0.6562 - val_acc: 0.7819\n","Epoch 20/100\n","40000/40000 [==============================] - 24s 588us/step - loss: 0.6457 - acc: 0.7860 - val_loss: 0.6407 - val_acc: 0.7886\n","Epoch 21/100\n","40000/40000 [==============================] - 24s 588us/step - loss: 0.6332 - acc: 0.7895 - val_loss: 0.6704 - val_acc: 0.7797\n","Epoch 22/100\n","40000/40000 [==============================] - 24s 590us/step - loss: 0.6178 - acc: 0.7963 - val_loss: 0.6585 - val_acc: 0.7808\n","Epoch 23/100\n","40000/40000 [==============================] - 24s 590us/step - loss: 0.6035 - acc: 0.7996 - val_loss: 0.6228 - val_acc: 0.7965\n","Epoch 24/100\n","40000/40000 [==============================] - 23s 587us/step - loss: 0.5954 - acc: 0.8033 - val_loss: 0.6300 - val_acc: 0.7952\n","Epoch 25/100\n","40000/40000 [==============================] - 24s 591us/step - loss: 0.5872 - acc: 0.8053 - val_loss: 0.6364 - val_acc: 0.7923\n","Epoch 26/100\n","40000/40000 [==============================] - 24s 589us/step - loss: 0.5710 - acc: 0.8110 - val_loss: 0.5914 - val_acc: 0.8028\n","Epoch 27/100\n","40000/40000 [==============================] - 24s 589us/step - loss: 0.5665 - acc: 0.8147 - val_loss: 0.6177 - val_acc: 0.7987\n","Epoch 28/100\n","40000/40000 [==============================] - 24s 589us/step - loss: 0.5536 - acc: 0.8176 - val_loss: 0.5741 - val_acc: 0.8119\n","Epoch 29/100\n","40000/40000 [==============================] - 24s 590us/step - loss: 0.5402 - acc: 0.8260 - val_loss: 0.6087 - val_acc: 0.8021\n","Epoch 30/100\n","40000/40000 [==============================] - 24s 590us/step - loss: 0.5334 - acc: 0.8264 - val_loss: 0.5801 - val_acc: 0.8069\n","Epoch 31/100\n","40000/40000 [==============================] - 24s 589us/step - loss: 0.5302 - acc: 0.8288 - val_loss: 0.5853 - val_acc: 0.8114\n","Epoch 32/100\n","40000/40000 [==============================] - 24s 590us/step - loss: 0.5198 - acc: 0.8310 - val_loss: 0.5845 - val_acc: 0.8110\n","Epoch 33/100\n","40000/40000 [==============================] - 24s 588us/step - loss: 0.5094 - acc: 0.8344 - val_loss: 0.6140 - val_acc: 0.8015\n","Epoch 34/100\n","40000/40000 [==============================] - 24s 590us/step - loss: 0.4999 - acc: 0.8381 - val_loss: 0.5554 - val_acc: 0.8172\n","Epoch 35/100\n","40000/40000 [==============================] - 24s 589us/step - loss: 0.4896 - acc: 0.8397 - val_loss: 0.5507 - val_acc: 0.8176\n","Epoch 36/100\n","40000/40000 [==============================] - 24s 589us/step - loss: 0.4871 - acc: 0.8408 - val_loss: 0.5634 - val_acc: 0.8199\n","Epoch 37/100\n","40000/40000 [==============================] - 23s 586us/step - loss: 0.4726 - acc: 0.8478 - val_loss: 0.5764 - val_acc: 0.8108\n","Epoch 38/100\n","40000/40000 [==============================] - 24s 590us/step - loss: 0.4748 - acc: 0.8461 - val_loss: 0.5445 - val_acc: 0.8234\n","Epoch 39/100\n","40000/40000 [==============================] - 24s 592us/step - loss: 0.4604 - acc: 0.8516 - val_loss: 0.5760 - val_acc: 0.8118\n","Epoch 40/100\n","40000/40000 [==============================] - 24s 590us/step - loss: 0.4595 - acc: 0.8481 - val_loss: 0.5693 - val_acc: 0.8133\n","Epoch 41/100\n","40000/40000 [==============================] - 24s 591us/step - loss: 0.4472 - acc: 0.8547 - val_loss: 0.5679 - val_acc: 0.8159\n","Epoch 42/100\n","40000/40000 [==============================] - 24s 591us/step - loss: 0.4464 - acc: 0.8569 - val_loss: 0.5436 - val_acc: 0.8275\n","Epoch 43/100\n","40000/40000 [==============================] - 24s 591us/step - loss: 0.4420 - acc: 0.8590 - val_loss: 0.5437 - val_acc: 0.8252\n","Epoch 44/100\n","40000/40000 [==============================] - 24s 590us/step - loss: 0.4361 - acc: 0.8591 - val_loss: 0.5219 - val_acc: 0.8311\n","Epoch 45/100\n","40000/40000 [==============================] - 24s 591us/step - loss: 0.4256 - acc: 0.8622 - val_loss: 0.5471 - val_acc: 0.8239\n","Epoch 46/100\n","40000/40000 [==============================] - 24s 590us/step - loss: 0.4187 - acc: 0.8664 - val_loss: 0.5253 - val_acc: 0.8300\n","Epoch 47/100\n","40000/40000 [==============================] - 24s 592us/step - loss: 0.4150 - acc: 0.8668 - val_loss: 0.5561 - val_acc: 0.8194\n","Epoch 48/100\n","40000/40000 [==============================] - 24s 591us/step - loss: 0.4099 - acc: 0.8696 - val_loss: 0.5227 - val_acc: 0.8346\n","Epoch 49/100\n","40000/40000 [==============================] - 24s 592us/step - loss: 0.4060 - acc: 0.8690 - val_loss: 0.5331 - val_acc: 0.8302\n","Epoch 50/100\n","40000/40000 [==============================] - 23s 587us/step - loss: 0.3980 - acc: 0.8723 - val_loss: 0.5103 - val_acc: 0.8360\n","Epoch 51/100\n","40000/40000 [==============================] - 24s 592us/step - loss: 0.3922 - acc: 0.8747 - val_loss: 0.5205 - val_acc: 0.8303\n","Epoch 52/100\n","40000/40000 [==============================] - 24s 591us/step - loss: 0.3847 - acc: 0.8768 - val_loss: 0.5054 - val_acc: 0.8388\n","Epoch 53/100\n","40000/40000 [==============================] - 24s 591us/step - loss: 0.3812 - acc: 0.8772 - val_loss: 0.5528 - val_acc: 0.8241\n","Epoch 54/100\n","40000/40000 [==============================] - 24s 591us/step - loss: 0.3830 - acc: 0.8777 - val_loss: 0.5406 - val_acc: 0.8275\n","Epoch 55/100\n","40000/40000 [==============================] - 24s 592us/step - loss: 0.3758 - acc: 0.8802 - val_loss: 0.5277 - val_acc: 0.8327\n","Epoch 56/100\n","40000/40000 [==============================] - 24s 592us/step - loss: 0.3693 - acc: 0.8835 - val_loss: 0.5486 - val_acc: 0.8282\n","Epoch 57/100\n","40000/40000 [==============================] - 24s 589us/step - loss: 0.3624 - acc: 0.8846 - val_loss: 0.5208 - val_acc: 0.8359\n","Epoch 58/100\n","40000/40000 [==============================] - 24s 591us/step - loss: 0.3640 - acc: 0.8836 - val_loss: 0.5316 - val_acc: 0.8355\n","Epoch 59/100\n","40000/40000 [==============================] - 24s 589us/step - loss: 0.3510 - acc: 0.8887 - val_loss: 0.5005 - val_acc: 0.8431\n","Epoch 60/100\n","40000/40000 [==============================] - 23s 587us/step - loss: 0.3495 - acc: 0.8881 - val_loss: 0.5043 - val_acc: 0.8405\n","Epoch 61/100\n","40000/40000 [==============================] - 24s 591us/step - loss: 0.3463 - acc: 0.8916 - val_loss: 0.5321 - val_acc: 0.8327\n","Epoch 62/100\n","40000/40000 [==============================] - 24s 591us/step - loss: 0.3438 - acc: 0.8903 - val_loss: 0.5148 - val_acc: 0.8397\n","Epoch 63/100\n","40000/40000 [==============================] - 23s 586us/step - loss: 0.3415 - acc: 0.8922 - val_loss: 0.5221 - val_acc: 0.8346\n","Epoch 64/100\n","40000/40000 [==============================] - 24s 594us/step - loss: 0.3320 - acc: 0.8954 - val_loss: 0.5156 - val_acc: 0.8368\n","Epoch 65/100\n","40000/40000 [==============================] - 24s 598us/step - loss: 0.3256 - acc: 0.8980 - val_loss: 0.5200 - val_acc: 0.8434\n","Epoch 66/100\n","40000/40000 [==============================] - 24s 597us/step - loss: 0.3306 - acc: 0.8957 - val_loss: 0.5120 - val_acc: 0.8401\n","Epoch 67/100\n","40000/40000 [==============================] - 24s 599us/step - loss: 0.3247 - acc: 0.8989 - val_loss: 0.5050 - val_acc: 0.8445\n","Epoch 68/100\n","40000/40000 [==============================] - 24s 597us/step - loss: 0.3182 - acc: 0.9009 - val_loss: 0.5433 - val_acc: 0.8316\n","Epoch 69/100\n","40000/40000 [==============================] - 24s 597us/step - loss: 0.3149 - acc: 0.9021 - val_loss: 0.5206 - val_acc: 0.8393\n","Epoch 70/100\n","40000/40000 [==============================] - 24s 597us/step - loss: 0.3150 - acc: 0.9026 - val_loss: 0.5075 - val_acc: 0.8401\n","Epoch 71/100\n","40000/40000 [==============================] - 24s 597us/step - loss: 0.3081 - acc: 0.9040 - val_loss: 0.5359 - val_acc: 0.8352\n","Epoch 72/100\n","40000/40000 [==============================] - 24s 595us/step - loss: 0.3074 - acc: 0.9056 - val_loss: 0.5409 - val_acc: 0.8359\n","Epoch 73/100\n","40000/40000 [==============================] - 24s 597us/step - loss: 0.2993 - acc: 0.9069 - val_loss: 0.5107 - val_acc: 0.8437\n","Epoch 74/100\n","40000/40000 [==============================] - 24s 596us/step - loss: 0.3015 - acc: 0.9054 - val_loss: 0.5102 - val_acc: 0.8435\n","Epoch 75/100\n","40000/40000 [==============================] - 24s 597us/step - loss: 0.2952 - acc: 0.9079 - val_loss: 0.5024 - val_acc: 0.8476\n","Epoch 76/100\n","40000/40000 [==============================] - 24s 593us/step - loss: 0.2944 - acc: 0.9085 - val_loss: 0.5264 - val_acc: 0.8405\n","Epoch 77/100\n","40000/40000 [==============================] - 24s 595us/step - loss: 0.2935 - acc: 0.9087 - val_loss: 0.5117 - val_acc: 0.8484\n","Epoch 78/100\n","40000/40000 [==============================] - 24s 597us/step - loss: 0.2929 - acc: 0.9089 - val_loss: 0.5001 - val_acc: 0.8465\n","Epoch 79/100\n","40000/40000 [==============================] - 24s 593us/step - loss: 0.2814 - acc: 0.9141 - val_loss: 0.5199 - val_acc: 0.8423\n","Epoch 80/100\n","40000/40000 [==============================] - 24s 597us/step - loss: 0.2821 - acc: 0.9133 - val_loss: 0.5096 - val_acc: 0.8490\n","Epoch 81/100\n","40000/40000 [==============================] - 24s 595us/step - loss: 0.2774 - acc: 0.9144 - val_loss: 0.4974 - val_acc: 0.8514\n","Epoch 82/100\n","40000/40000 [==============================] - 24s 595us/step - loss: 0.2757 - acc: 0.9157 - val_loss: 0.5029 - val_acc: 0.8492\n","Epoch 83/100\n","40000/40000 [==============================] - 24s 594us/step - loss: 0.2725 - acc: 0.9164 - val_loss: 0.5100 - val_acc: 0.8446\n","Epoch 84/100\n","40000/40000 [==============================] - 24s 595us/step - loss: 0.2666 - acc: 0.9183 - val_loss: 0.5341 - val_acc: 0.8410\n","Epoch 85/100\n","40000/40000 [==============================] - 24s 594us/step - loss: 0.2711 - acc: 0.9157 - val_loss: 0.5054 - val_acc: 0.8486\n","Epoch 86/100\n","40000/40000 [==============================] - 24s 595us/step - loss: 0.2684 - acc: 0.9182 - val_loss: 0.5136 - val_acc: 0.8471\n","Epoch 87/100\n","40000/40000 [==============================] - 24s 597us/step - loss: 0.2642 - acc: 0.9198 - val_loss: 0.5081 - val_acc: 0.8501\n","Epoch 88/100\n","40000/40000 [==============================] - 24s 598us/step - loss: 0.2583 - acc: 0.9219 - val_loss: 0.5112 - val_acc: 0.8504\n","Epoch 89/100\n","40000/40000 [==============================] - 24s 593us/step - loss: 0.2553 - acc: 0.9231 - val_loss: 0.5541 - val_acc: 0.8395\n","Epoch 90/100\n","40000/40000 [==============================] - 24s 598us/step - loss: 0.2582 - acc: 0.9207 - val_loss: 0.5224 - val_acc: 0.8478\n","Epoch 91/100\n","40000/40000 [==============================] - 24s 598us/step - loss: 0.2528 - acc: 0.9242 - val_loss: 0.5361 - val_acc: 0.8435\n","Epoch 92/100\n","40000/40000 [==============================] - 24s 598us/step - loss: 0.2497 - acc: 0.9255 - val_loss: 0.5288 - val_acc: 0.8500\n","Epoch 93/100\n","40000/40000 [==============================] - 24s 597us/step - loss: 0.2517 - acc: 0.9244 - val_loss: 0.5083 - val_acc: 0.8504\n","Epoch 94/100\n","40000/40000 [==============================] - 24s 589us/step - loss: 0.2464 - acc: 0.9258 - val_loss: 0.5226 - val_acc: 0.8519\n","Epoch 95/100\n","40000/40000 [==============================] - 24s 590us/step - loss: 0.2485 - acc: 0.9253 - val_loss: 0.5236 - val_acc: 0.8509\n","Epoch 96/100\n","40000/40000 [==============================] - 24s 588us/step - loss: 0.2457 - acc: 0.9247 - val_loss: 0.5465 - val_acc: 0.8462\n","Epoch 97/100\n","40000/40000 [==============================] - 23s 584us/step - loss: 0.2370 - acc: 0.9306 - val_loss: 0.5199 - val_acc: 0.8503\n","Epoch 98/100\n","40000/40000 [==============================] - 23s 585us/step - loss: 0.2402 - acc: 0.9295 - val_loss: 0.5207 - val_acc: 0.8525\n","Epoch 99/100\n","40000/40000 [==============================] - 23s 586us/step - loss: 0.2388 - acc: 0.9274 - val_loss: 0.5365 - val_acc: 0.8415\n","Epoch 100/100\n","40000/40000 [==============================] - 23s 586us/step - loss: 0.2381 - acc: 0.9295 - val_loss: 0.5237 - val_acc: 0.8535\n"],"name":"stdout"}]},{"metadata":{"id":"EYmDDyAYxMf6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":279},"outputId":"76025af8-4a5e-4a2b-adab-69f7c8b2c516","executionInfo":{"status":"ok","timestamp":1554674929493,"user_tz":240,"elapsed":838,"user":{"displayName":"Sushmith Ramesh","photoUrl":"https://lh3.googleusercontent.com/-bVb_Tqiu7NU/AAAAAAAAAAI/AAAAAAAAAqU/mfZ6cPSstVg/s64/photo.jpg","userId":"13694073694977092311"}}},"cell_type":"code","source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, 'bo', label='Training acc')\n","plt.plot(epochs, val_acc, 'r', label='Validation acc')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"],"execution_count":62,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"jdY5_OwwxMf9","colab_type":"text"},"cell_type":"markdown","source":["## 3. Train (again) and evaluate the model\n","\n","- To this end, you have found the \"best\" hyper-parameters. \n","- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n","- Evaluate your model on the test set."]},{"metadata":{"id":"0A09EBRMxMf-","colab_type":"text"},"cell_type":"markdown","source":["### 3.1. Train the model on the entire training set\n","\n","Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"]},{"metadata":{"id":"qo2M_VFoxMf-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1088},"outputId":"5929927b-0255-4314-b6cf-ad088df6c507","executionInfo":{"status":"ok","timestamp":1554674954743,"user_tz":240,"elapsed":1483,"user":{"displayName":"Sushmith Ramesh","photoUrl":"https://lh3.googleusercontent.com/-bVb_Tqiu7NU/AAAAAAAAAAI/AAAAAAAAAqU/mfZ6cPSstVg/s64/photo.jpg","userId":"13694073694977092311"}}},"cell_type":"code","source":["# <Compile your model again (using the same hyper-parameters)>\n","\n","from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\n","from keras.models import Sequential\n","\n","weight_decay = 1e-4\n","model = Sequential()\n","model.add(Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay), input_shape=(32, 32, 3)))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Conv2D(32, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D((2, 2)))\n","\n","model.add(Dropout(0.2))\n","\n","model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Conv2D(64, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D((2, 2)))\n","\n","model.add(Dropout(0.3))\n","\n","model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(Conv2D(128, (3, 3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))\n","model.add(BatchNormalization())\n","model.add(Activation('relu'))\n","model.add(MaxPooling2D((2, 2)))\n","\n","model.add(Dropout(0.4))\n","\n","model.add(Flatten())\n","model.add(Dense(128, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dense(10, activation='softmax'))\n","\n","model.summary()\n","\n","from keras import optimizers\n","\n","learning_rate = 1E-4 # to be tuned!\n","\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=optimizers.RMSprop(lr=learning_rate),\n","              metrics=['acc'])"],"execution_count":63,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_57 (Conv2D)           (None, 32, 32, 32)        896       \n","_________________________________________________________________\n","batch_normalization_70 (Batc (None, 32, 32, 32)        128       \n","_________________________________________________________________\n","activation_57 (Activation)   (None, 32, 32, 32)        0         \n","_________________________________________________________________\n","conv2d_58 (Conv2D)           (None, 32, 32, 32)        9248      \n","_________________________________________________________________\n","batch_normalization_71 (Batc (None, 32, 32, 32)        128       \n","_________________________________________________________________\n","activation_58 (Activation)   (None, 32, 32, 32)        0         \n","_________________________________________________________________\n","max_pooling2d_51 (MaxPooling (None, 16, 16, 32)        0         \n","_________________________________________________________________\n","dropout_18 (Dropout)         (None, 16, 16, 32)        0         \n","_________________________________________________________________\n","conv2d_59 (Conv2D)           (None, 16, 16, 64)        18496     \n","_________________________________________________________________\n","batch_normalization_72 (Batc (None, 16, 16, 64)        256       \n","_________________________________________________________________\n","activation_59 (Activation)   (None, 16, 16, 64)        0         \n","_________________________________________________________________\n","conv2d_60 (Conv2D)           (None, 16, 16, 64)        36928     \n","_________________________________________________________________\n","batch_normalization_73 (Batc (None, 16, 16, 64)        256       \n","_________________________________________________________________\n","activation_60 (Activation)   (None, 16, 16, 64)        0         \n","_________________________________________________________________\n","max_pooling2d_52 (MaxPooling (None, 8, 8, 64)          0         \n","_________________________________________________________________\n","dropout_19 (Dropout)         (None, 8, 8, 64)          0         \n","_________________________________________________________________\n","conv2d_61 (Conv2D)           (None, 8, 8, 128)         73856     \n","_________________________________________________________________\n","batch_normalization_74 (Batc (None, 8, 8, 128)         512       \n","_________________________________________________________________\n","activation_61 (Activation)   (None, 8, 8, 128)         0         \n","_________________________________________________________________\n","conv2d_62 (Conv2D)           (None, 8, 8, 128)         147584    \n","_________________________________________________________________\n","batch_normalization_75 (Batc (None, 8, 8, 128)         512       \n","_________________________________________________________________\n","activation_62 (Activation)   (None, 8, 8, 128)         0         \n","_________________________________________________________________\n","max_pooling2d_53 (MaxPooling (None, 4, 4, 128)         0         \n","_________________________________________________________________\n","dropout_20 (Dropout)         (None, 4, 4, 128)         0         \n","_________________________________________________________________\n","flatten_14 (Flatten)         (None, 2048)              0         \n","_________________________________________________________________\n","dense_27 (Dense)             (None, 128)               262272    \n","_________________________________________________________________\n","batch_normalization_76 (Batc (None, 128)               512       \n","_________________________________________________________________\n","dense_28 (Dense)             (None, 10)                1290      \n","=================================================================\n","Total params: 552,874\n","Trainable params: 551,722\n","Non-trainable params: 1,152\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"OUFx6C1qxMgB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":3434},"outputId":"d7ac71ad-23ff-4912-e4db-31c3f17e94f7","executionInfo":{"status":"ok","timestamp":1554678227767,"user_tz":240,"elapsed":3264142,"user":{"displayName":"Sushmith Ramesh","photoUrl":"https://lh3.googleusercontent.com/-bVb_Tqiu7NU/AAAAAAAAAAI/AAAAAAAAAqU/mfZ6cPSstVg/s64/photo.jpg","userId":"13694073694977092311"}}},"cell_type":"code","source":["# <Train your model on the entire training set (50K samples)>\n","# <Use (x_train, y_train_vec) instead of (x_tr, y_tr)>\n","# <Do NOT use the validation_data option (because now you do not have validation data)>\n","\n","model.fit(x_train, y_train_vec, batch_size=64, epochs=100)"],"execution_count":64,"outputs":[{"output_type":"stream","text":["Epoch 1/100\n","50000/50000 [==============================] - 38s 770us/step - loss: 1.9076 - acc: 0.3532\n","Epoch 2/100\n","50000/50000 [==============================] - 34s 680us/step - loss: 1.4061 - acc: 0.5044\n","Epoch 3/100\n","50000/50000 [==============================] - 34s 680us/step - loss: 1.2239 - acc: 0.5769\n","Epoch 4/100\n","50000/50000 [==============================] - 34s 680us/step - loss: 1.1063 - acc: 0.6193\n","Epoch 5/100\n","50000/50000 [==============================] - 34s 679us/step - loss: 1.0314 - acc: 0.6445\n","Epoch 6/100\n","50000/50000 [==============================] - 33s 669us/step - loss: 0.9672 - acc: 0.6697\n","Epoch 7/100\n","50000/50000 [==============================] - 34s 670us/step - loss: 0.9163 - acc: 0.6889\n","Epoch 8/100\n","50000/50000 [==============================] - 33s 670us/step - loss: 0.8709 - acc: 0.7081\n","Epoch 9/100\n","50000/50000 [==============================] - 34s 674us/step - loss: 0.8435 - acc: 0.7172\n","Epoch 10/100\n","50000/50000 [==============================] - 33s 670us/step - loss: 0.8039 - acc: 0.7307\n","Epoch 11/100\n","50000/50000 [==============================] - 33s 665us/step - loss: 0.7776 - acc: 0.7392\n","Epoch 12/100\n","50000/50000 [==============================] - 33s 663us/step - loss: 0.7475 - acc: 0.7492\n","Epoch 13/100\n","50000/50000 [==============================] - 33s 663us/step - loss: 0.7287 - acc: 0.7563\n","Epoch 14/100\n","50000/50000 [==============================] - 33s 665us/step - loss: 0.7051 - acc: 0.7638\n","Epoch 15/100\n","50000/50000 [==============================] - 33s 661us/step - loss: 0.6917 - acc: 0.7696\n","Epoch 16/100\n","50000/50000 [==============================] - 33s 664us/step - loss: 0.6765 - acc: 0.7768\n","Epoch 17/100\n","50000/50000 [==============================] - 33s 666us/step - loss: 0.6544 - acc: 0.7842\n","Epoch 18/100\n","50000/50000 [==============================] - 33s 664us/step - loss: 0.6462 - acc: 0.7865\n","Epoch 19/100\n","50000/50000 [==============================] - 33s 659us/step - loss: 0.6274 - acc: 0.7929\n","Epoch 20/100\n","50000/50000 [==============================] - 33s 662us/step - loss: 0.6144 - acc: 0.7973\n","Epoch 21/100\n","50000/50000 [==============================] - 33s 662us/step - loss: 0.6047 - acc: 0.8003\n","Epoch 22/100\n","50000/50000 [==============================] - 33s 660us/step - loss: 0.5911 - acc: 0.8060\n","Epoch 23/100\n","50000/50000 [==============================] - 33s 661us/step - loss: 0.5787 - acc: 0.8110\n","Epoch 24/100\n","50000/50000 [==============================] - 33s 662us/step - loss: 0.5723 - acc: 0.8114\n","Epoch 25/100\n","50000/50000 [==============================] - 33s 660us/step - loss: 0.5570 - acc: 0.8178\n","Epoch 26/100\n","50000/50000 [==============================] - 34s 671us/step - loss: 0.5497 - acc: 0.8189\n","Epoch 27/100\n","50000/50000 [==============================] - 34s 672us/step - loss: 0.5354 - acc: 0.8251\n","Epoch 28/100\n","50000/50000 [==============================] - 33s 667us/step - loss: 0.5247 - acc: 0.8294\n","Epoch 29/100\n","50000/50000 [==============================] - 34s 670us/step - loss: 0.5192 - acc: 0.8307\n","Epoch 30/100\n","50000/50000 [==============================] - 34s 670us/step - loss: 0.5087 - acc: 0.8341\n","Epoch 31/100\n","50000/50000 [==============================] - 34s 673us/step - loss: 0.5048 - acc: 0.8370\n","Epoch 32/100\n","50000/50000 [==============================] - 34s 673us/step - loss: 0.4890 - acc: 0.8405\n","Epoch 33/100\n","50000/50000 [==============================] - 33s 664us/step - loss: 0.4883 - acc: 0.8428\n","Epoch 34/100\n","50000/50000 [==============================] - 33s 662us/step - loss: 0.4788 - acc: 0.8451\n","Epoch 35/100\n","50000/50000 [==============================] - 33s 663us/step - loss: 0.4735 - acc: 0.8481\n","Epoch 36/100\n","50000/50000 [==============================] - 33s 663us/step - loss: 0.4633 - acc: 0.8508\n","Epoch 37/100\n","50000/50000 [==============================] - 33s 663us/step - loss: 0.4528 - acc: 0.8541\n","Epoch 38/100\n","50000/50000 [==============================] - 33s 659us/step - loss: 0.4513 - acc: 0.8544\n","Epoch 39/100\n","50000/50000 [==============================] - 33s 664us/step - loss: 0.4418 - acc: 0.8593\n","Epoch 40/100\n","50000/50000 [==============================] - 33s 664us/step - loss: 0.4366 - acc: 0.8589\n","Epoch 41/100\n","50000/50000 [==============================] - 33s 664us/step - loss: 0.4334 - acc: 0.8609\n","Epoch 42/100\n","50000/50000 [==============================] - 33s 664us/step - loss: 0.4256 - acc: 0.8633\n","Epoch 43/100\n","50000/50000 [==============================] - 33s 659us/step - loss: 0.4190 - acc: 0.8678\n","Epoch 44/100\n","50000/50000 [==============================] - 33s 651us/step - loss: 0.4134 - acc: 0.8668\n","Epoch 45/100\n","50000/50000 [==============================] - 32s 649us/step - loss: 0.4085 - acc: 0.8693\n","Epoch 46/100\n","50000/50000 [==============================] - 32s 649us/step - loss: 0.4022 - acc: 0.8715\n","Epoch 47/100\n","50000/50000 [==============================] - 32s 647us/step - loss: 0.3999 - acc: 0.8725\n","Epoch 48/100\n","50000/50000 [==============================] - 33s 652us/step - loss: 0.3933 - acc: 0.8744\n","Epoch 49/100\n","50000/50000 [==============================] - 33s 652us/step - loss: 0.3888 - acc: 0.8763\n","Epoch 50/100\n","50000/50000 [==============================] - 33s 652us/step - loss: 0.3836 - acc: 0.8793\n","Epoch 51/100\n","50000/50000 [==============================] - 33s 651us/step - loss: 0.3747 - acc: 0.8803\n","Epoch 52/100\n","50000/50000 [==============================] - 32s 650us/step - loss: 0.3741 - acc: 0.8805\n","Epoch 53/100\n","50000/50000 [==============================] - 33s 651us/step - loss: 0.3686 - acc: 0.8832\n","Epoch 54/100\n","50000/50000 [==============================] - 33s 650us/step - loss: 0.3648 - acc: 0.8839\n","Epoch 55/100\n","50000/50000 [==============================] - 33s 652us/step - loss: 0.3586 - acc: 0.8848\n","Epoch 56/100\n","50000/50000 [==============================] - 33s 652us/step - loss: 0.3526 - acc: 0.8889\n","Epoch 57/100\n","50000/50000 [==============================] - 33s 651us/step - loss: 0.3470 - acc: 0.8900\n","Epoch 58/100\n","50000/50000 [==============================] - 33s 654us/step - loss: 0.3447 - acc: 0.8901\n","Epoch 59/100\n","50000/50000 [==============================] - 33s 653us/step - loss: 0.3445 - acc: 0.8922\n","Epoch 60/100\n","50000/50000 [==============================] - 33s 653us/step - loss: 0.3374 - acc: 0.8938\n","Epoch 61/100\n","50000/50000 [==============================] - 32s 645us/step - loss: 0.3312 - acc: 0.8960\n","Epoch 62/100\n","50000/50000 [==============================] - 32s 639us/step - loss: 0.3324 - acc: 0.8958\n","Epoch 63/100\n","50000/50000 [==============================] - 32s 643us/step - loss: 0.3243 - acc: 0.8980\n","Epoch 64/100\n","50000/50000 [==============================] - 32s 642us/step - loss: 0.3216 - acc: 0.8993\n","Epoch 65/100\n","50000/50000 [==============================] - 32s 643us/step - loss: 0.3225 - acc: 0.9006\n","Epoch 66/100\n","50000/50000 [==============================] - 32s 640us/step - loss: 0.3157 - acc: 0.9005\n","Epoch 67/100\n","50000/50000 [==============================] - 32s 641us/step - loss: 0.3147 - acc: 0.9027\n","Epoch 68/100\n","50000/50000 [==============================] - 32s 641us/step - loss: 0.3078 - acc: 0.9037\n","Epoch 69/100\n","50000/50000 [==============================] - 32s 642us/step - loss: 0.3051 - acc: 0.9058\n","Epoch 70/100\n","50000/50000 [==============================] - 32s 637us/step - loss: 0.3025 - acc: 0.9056\n","Epoch 71/100\n","50000/50000 [==============================] - 32s 631us/step - loss: 0.3004 - acc: 0.9068\n","Epoch 72/100\n","50000/50000 [==============================] - 32s 633us/step - loss: 0.2989 - acc: 0.9077\n","Epoch 73/100\n","50000/50000 [==============================] - 32s 633us/step - loss: 0.2966 - acc: 0.9096\n","Epoch 74/100\n","50000/50000 [==============================] - 32s 632us/step - loss: 0.2914 - acc: 0.9095\n","Epoch 75/100\n","50000/50000 [==============================] - 32s 634us/step - loss: 0.2875 - acc: 0.9120\n","Epoch 76/100\n","50000/50000 [==============================] - 31s 630us/step - loss: 0.2872 - acc: 0.9108\n","Epoch 77/100\n","50000/50000 [==============================] - 32s 633us/step - loss: 0.2827 - acc: 0.9121\n","Epoch 78/100\n","50000/50000 [==============================] - 32s 633us/step - loss: 0.2827 - acc: 0.9129\n","Epoch 79/100\n","50000/50000 [==============================] - 32s 633us/step - loss: 0.2744 - acc: 0.9157\n","Epoch 80/100\n","50000/50000 [==============================] - 32s 634us/step - loss: 0.2756 - acc: 0.9159\n","Epoch 81/100\n","50000/50000 [==============================] - 32s 633us/step - loss: 0.2730 - acc: 0.9157\n","Epoch 82/100\n","50000/50000 [==============================] - 32s 634us/step - loss: 0.2743 - acc: 0.9162\n","Epoch 83/100\n","50000/50000 [==============================] - 32s 635us/step - loss: 0.2696 - acc: 0.9184\n","Epoch 84/100\n","50000/50000 [==============================] - 32s 634us/step - loss: 0.2672 - acc: 0.9188\n","Epoch 85/100\n","50000/50000 [==============================] - 32s 635us/step - loss: 0.2667 - acc: 0.9194\n","Epoch 86/100\n","50000/50000 [==============================] - 32s 631us/step - loss: 0.2603 - acc: 0.9210\n","Epoch 87/100\n","50000/50000 [==============================] - 32s 634us/step - loss: 0.2595 - acc: 0.9214\n","Epoch 88/100\n","50000/50000 [==============================] - 32s 635us/step - loss: 0.2601 - acc: 0.9205\n","Epoch 89/100\n","50000/50000 [==============================] - 32s 634us/step - loss: 0.2551 - acc: 0.9232\n","Epoch 90/100\n","50000/50000 [==============================] - 32s 634us/step - loss: 0.2542 - acc: 0.9246\n","Epoch 91/100\n","50000/50000 [==============================] - 32s 631us/step - loss: 0.2509 - acc: 0.9245\n","Epoch 92/100\n","50000/50000 [==============================] - 32s 634us/step - loss: 0.2515 - acc: 0.9241\n","Epoch 93/100\n","50000/50000 [==============================] - 32s 635us/step - loss: 0.2518 - acc: 0.9242\n","Epoch 94/100\n","50000/50000 [==============================] - 32s 635us/step - loss: 0.2486 - acc: 0.9249\n","Epoch 95/100\n","50000/50000 [==============================] - 32s 633us/step - loss: 0.2473 - acc: 0.9273\n","Epoch 96/100\n","50000/50000 [==============================] - 32s 631us/step - loss: 0.2423 - acc: 0.9269\n","Epoch 97/100\n","50000/50000 [==============================] - 32s 635us/step - loss: 0.2438 - acc: 0.9267\n","Epoch 98/100\n","50000/50000 [==============================] - 32s 635us/step - loss: 0.2372 - acc: 0.9296\n","Epoch 99/100\n","50000/50000 [==============================] - 32s 634us/step - loss: 0.2340 - acc: 0.9311\n","Epoch 100/100\n","50000/50000 [==============================] - 32s 634us/step - loss: 0.2356 - acc: 0.9304\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f55e03a3d30>"]},"metadata":{"tags":[]},"execution_count":64}]},{"metadata":{"id":"axdVx_b7xMgE","colab_type":"text"},"cell_type":"markdown","source":["### 3.2. Evaluate the model on the test set\n","\n","Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."]},{"metadata":{"id":"PNdAAkv_xMgF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"856b136f-3b96-4b7a-8489-2bb2b26dc4ab","executionInfo":{"status":"ok","timestamp":1554678237574,"user_tz":240,"elapsed":4511,"user":{"displayName":"Sushmith Ramesh","photoUrl":"https://lh3.googleusercontent.com/-bVb_Tqiu7NU/AAAAAAAAAAI/AAAAAAAAAqU/mfZ6cPSstVg/s64/photo.jpg","userId":"13694073694977092311"}}},"cell_type":"code","source":["loss_and_acc = model.evaluate(x_test, y_test_vec)\n","print('loss = ' + str(loss_and_acc[0]))\n","print('accuracy = ' + str(loss_and_acc[1]))"],"execution_count":65,"outputs":[{"output_type":"stream","text":["10000/10000 [==============================] - 4s 406us/step\n","loss = 0.5198424721479415\n","accuracy = 0.8536\n"],"name":"stdout"}]},{"metadata":{"id":"QoSdFLNwxMgI","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}